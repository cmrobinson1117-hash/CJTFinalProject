{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cmqdj3ug4mr4tgkbc65p",
   "authorId": "4401448276033",
   "authorName": "CHIPMUNK",
   "authorEmail": "",
   "sessionId": "258460d6-3196-4a33-ae29-5383a5016292",
   "lastEditTime": 1771530929078
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b84ab4ca-8dff-461a-b204-28a2076a739f",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "Phase 2"
  },
  {
   "cell_type": "code",
   "id": "75d6b45a-9b4c-4dd3-83e0-fa03d0f348b8",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "import os\nos.listdir()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3413b478-c185-415f-bbe5-434ebb29a29d",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "import pandas as pd\n\ntrain = pd.read_csv(\"train.csv\")\nfeatures = pd.read_csv(\"features.csv\")\nstores = pd.read_csv(\"stores.csv\")\n\ntrain.head()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42ac3bf3-faeb-448d-993f-8a98bb49d6e1",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nsession.write_pandas(train, \"TRAIN_RAW\", auto_create_table=True, overwrite=True)\nsession.write_pandas(features, \"FEATURES_RAW\", auto_create_table=True, overwrite=True)\nsession.write_pandas(stores, \"STORES_RAW\", auto_create_table=True, overwrite=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17ce83db-5581-4d95-a86e-ce90a304d205",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "SELECT 'TRAIN_RAW' AS T, COUNT(*) AS N FROM TRAIN_RAW\nUNION ALL\nSELECT 'FEATURES_RAW', COUNT(*) FROM FEATURES_RAW\nUNION ALL\nSELECT 'STORES_RAW', COUNT(*) FROM STORES_RAW;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cba9912c-c082-42f6-882f-1c83476fff4d",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "DESC TABLE TRAIN_RAW;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe4e429d-0cfc-4409-9dd9-f7c63e6d1fb3",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE STORE_WEEKLY_SALES AS\nSELECT\n    t.\"Date\" AS DS,\n    SUM(t.\"Weekly_Sales\") AS Y,\n    MAX(IFF(t.\"IsHoliday\", 1, 0)) AS IS_HOLIDAY,\n    AVG(f.\"Temperature\") AS TEMPERATURE,\n    AVG(f.\"Fuel_Price\") AS FUEL_PRICE,\n    AVG(f.\"CPI\") AS CPI,\n    AVG(f.\"Unemployment\") AS UNEMPLOYMENT\nFROM TRAIN_RAW t\nLEFT JOIN FEATURES_RAW f\n  ON t.\"Store\" = f.\"Store\"\n AND t.\"Date\"  = f.\"Date\"\nWHERE t.\"Store\" = 1   \nGROUP BY t.\"Date\"\nORDER BY DS;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1bd1ec6-6948-4f4b-a869-a5318e0c8f48",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "SELECT COUNT(*) FROM STORE_WEEKLY_SALES;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b565d93-5e08-4026-a8f0-5c625d0b1ac7",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "SELECT * FROM STORE_WEEKLY_SALES\nORDER BY DS\nLIMIT 10;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29473e27-5db7-44d2-8ea0-55b05cbef841",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nsession = get_active_session()\n\ndf = session.table(\"STORE_WEEKLY_SALES\").to_pandas()\ndf[\"DS\"] = pd.to_datetime(df[\"DS\"])\ndf = df.sort_values(\"DS\").set_index(\"DS\")\n\ndf.head()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d7b48a0-fa1d-44ca-98d3-9892630f121f",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "Phase 3"
  },
  {
   "cell_type": "code",
   "id": "b65e9c4d-71bd-48f3-aabe-09647a0bf0ad",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nplt.figure()\nplt.plot(df.index, df[\"Y\"])\nplt.title(\"Weekly Total Sales - Store 1\")\nplt.xlabel(\"Week\")\nplt.ylabel(\"Total Weekly Sales\")\n\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))      # every 3 months\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))      # label format\n\nplt.gcf().autofmt_xdate()   # rotates/aligns labels\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42fea190-6aee-445b-9961-6be653041e13",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndf[\"rolling_mean_12\"] = df[\"Y\"].rolling(window=12).mean()\n\nplt.figure()\nplt.plot(df.index, df[\"Y\"], label=\"Weekly Sales\", alpha=0.5)\nplt.plot(df.index, df[\"rolling_mean_12\"], label=\"12-Week Rolling Mean\", linewidth=2)\nplt.title(\"Weekly Sales with 12-Week Rolling Mean\")\nplt.xlabel(\"Week\")\nplt.ylabel(\"Total Weekly Sales\")\nplt.legend()\n\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n\nplt.gcf().autofmt_xdate()\nplt.tight_layout()\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53d186be-b234-41f3-ba24-6ee53ee54de8",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "holiday_avg = df.groupby(\"IS_HOLIDAY\")[\"Y\"].mean()\nholiday_avg\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44aff441-9278-48c4-b478-733bf3ff740e",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "holiday_avg.plot(kind=\"bar\")\nplt.title(\"Average Weekly Sales: Holiday vs Non-Holiday\")\nplt.ylabel(\"Average Weekly Sales\")\nplt.xticks([0,1], [\"Non-Holiday\", \"Holiday\"], rotation=0)\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e1b59e2-9a38-4a35-a8a7-fa40c6a89527",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom statsmodels.tsa.seasonal import STL\n\n# Safety: make sure the index is datetime + sorted (doesn't change your values)\ndf.index = pd.to_datetime(df.index)\ndf = df.sort_index()\n\n# STL decomposition (weekly data -> yearly seasonality ~ 52 weeks)\nstl = STL(df[\"Y\"], period=52)\nresult = stl.fit()\n\n# Plot + fix the crowded x-axis labels on all subplots\nfig = result.plot()\n\nfor ax in fig.axes:\n    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))     # every 3 months\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))     # YYYY-MM\n\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5e3abb9-f6cf-497e-bbfd-85dbd4112dc2",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "from statsmodels.tsa.stattools import adfuller\n\nadf_result = adfuller(df[\"Y\"])\n\nprint(\"ADF Statistic:\", adf_result[0])\nprint(\"p-value:\", adf_result[1])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdc6f94c-83d7-4559-916c-95d4be60a81a",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10,4))\nplot_acf(df[\"Y\"], lags=60)\nplt.show()\n\nplt.figure(figsize=(10,4))\nplot_pacf(df[\"Y\"], lags=60)\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "337c0442-0c4b-4154-9eb0-3da932ea84cc",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "Phase 4"
  },
  {
   "cell_type": "code",
   "id": "de189708-0f27-4777-880b-2bc4b5b4c7d0",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nimport pandas as pd\n\nsession = get_active_session()\n\n# Pull your engineered table into pandas\ndf = session.table(\"STORE_WEEKLY_SALES\").to_pandas()\n\n# Ensure DS is datetime and used as index\ndf[\"DS\"] = pd.to_datetime(df[\"DS\"])\ndf = df.sort_values(\"DS\").set_index(\"DS\")\n\n# Quick sanity check\ndf.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62f26863-9f07-4c4e-ba64-550287c1f7d9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\n\n# 12-week forecast horizon\nh = 12\n\n# Target series: weekly on Fridays (matches your data)\ny = df[\"Y\"].asfreq(\"W-FRI\")\n\n# Handle any missing weeks (rare but possible)\n# (this does NOT change existing values; it only fills missing timestamps if any exist)\ny = y.interpolate(\"time\").ffill().bfill()\n\n# Exogenous features for SARIMAX (advanced model)\nexog_cols = [\"IS_HOLIDAY\", \"TEMPERATURE\", \"FUEL_PRICE\", \"CPI\", \"UNEMPLOYMENT\"]\nX = df[exog_cols].asfreq(\"W-FRI\")\nX = X.interpolate(\"time\").ffill().bfill()\n\n# Train/test split (last 12 weeks as test)\ntrain_y = y.iloc[:-h]\ntest_y  = y.iloc[-h:]\n\ntrain_X = X.loc[train_y.index]\ntest_X  = X.loc[test_y.index]\n\n# Show shapes + date ranges\ntrain_y.shape, test_y.shape, (train_y.index.min(), train_y.index.max(), test_y.index.min(), test_y.index.max())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4775264-9619-43bd-b98d-df0103340f1d",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)  # hides the \"inferred frequency W-FRI\" spam\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# -----------------------------\n# Make sure freq is explicit (prevents warnings + keeps indexing consistent)\n# Assumes you already created train_y, test_y, train_X, test_X in the prior cell.\n# -----------------------------\ntrain_y = train_y.asfreq(\"W-FRI\")\ntest_y  = test_y.asfreq(\"W-FRI\")\ntrain_X = train_X.asfreq(\"W-FRI\")\ntest_X  = test_X.asfreq(\"W-FRI\")\n\ntrain_y.index.freq = \"W-FRI\"\ntest_y.index.freq  = \"W-FRI\"\ntrain_X.index.freq = \"W-FRI\"\ntest_X.index.freq  = \"W-FRI\"\n\n# -----------------------------\n# Metrics helpers\n# -----------------------------\ndef mape(y_true, y_pred):\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    eps = 1e-9\n    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n\ndef eval_metrics(y_true, y_pred, label):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n    mape_val = mape(y_true, y_pred)\n    return {\"model\": label, \"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape_val}\n\n# ---------------------------------------\n# Walk-forward validation (12-step)\n# Expanding window, 1-step ahead repeated\n# ---------------------------------------\ndef walk_forward_ets(train_y, test_y):\n    history = train_y.copy()\n    preds = []\n    for t in range(len(test_y)):\n        model = ExponentialSmoothing(history, trend=\"add\", seasonal=None).fit(optimized=True)\n        yhat = model.forecast(1).iloc[0]\n        preds.append(yhat)\n        history = pd.concat([history, pd.Series([test_y.iloc[t]], index=[test_y.index[t]])])\n        history.index.freq = \"W-FRI\"\n    return np.array(preds)\n\ndef walk_forward_sarima(train_y, test_y, order=(1,0,1), seasonal_order=(0,1,0,52)):\n    history = train_y.copy()\n    preds = []\n    for t in range(len(test_y)):\n        model = SARIMAX(\n            history,\n            order=order,\n            seasonal_order=seasonal_order,\n            enforce_stationarity=False,\n            enforce_invertibility=False\n        ).fit(disp=False)\n        yhat = model.forecast(1).iloc[0]\n        preds.append(yhat)\n        history = pd.concat([history, pd.Series([test_y.iloc[t]], index=[test_y.index[t]])])\n        history.index.freq = \"W-FRI\"\n    return np.array(preds)\n\ndef walk_forward_sarimax(train_y, test_y, train_X, test_X, order=(1,0,1), seasonal_order=(0,1,0,52)):\n    history_y = train_y.copy()\n    history_X = train_X.copy()\n    preds = []\n\n    for t in range(len(test_y)):\n        model = SARIMAX(\n            history_y,\n            exog=history_X,\n            order=order,\n            seasonal_order=seasonal_order,\n            enforce_stationarity=False,\n            enforce_invertibility=False\n        ).fit(disp=False)\n\n        yhat = model.forecast(1, exog=test_X.iloc[[t]]).iloc[0]\n        preds.append(yhat)\n\n        history_y = pd.concat([history_y, pd.Series([test_y.iloc[t]], index=[test_y.index[t]])])\n        history_X = pd.concat([history_X, test_X.iloc[[t]]])\n\n        history_y.index.freq = \"W-FRI\"\n        history_X.index.freq = \"W-FRI\"\n\n    return np.array(preds)\n\n# -----------------------------\n# 1) Baseline model: ETS\n# -----------------------------\nets_preds = walk_forward_ets(train_y, test_y)\n\n# -----------------------------\n# 2) Baseline model: SARIMA\n# -----------------------------\nsarima_order = (1, 0, 1)\nsarima_seasonal = (0, 1, 0, 52)\nsarima_preds = walk_forward_sarima(train_y, test_y, order=sarima_order, seasonal_order=sarima_seasonal)\n\n# -----------------------------\n# 3) Advanced model: SARIMAX (with exogenous features)\n# -----------------------------\nsarimax_order = (1, 0, 1)\nsarimax_seasonal = (0, 1, 0, 52)\nsarimax_preds = walk_forward_sarimax(\n    train_y, test_y,\n    train_X, test_X,\n    order=sarimax_order,\n    seasonal_order=sarimax_seasonal\n)\n\n# -----------------------------\n# Evaluation table\n# -----------------------------\nresults = [\n    eval_metrics(test_y.values, ets_preds, \"ETS (baseline)\"),\n    eval_metrics(test_y.values, sarima_preds, \"SARIMA (baseline)\"),\n    eval_metrics(test_y.values, sarimax_preds, \"SARIMAX + exog (advanced)\")\n]\neval_df = pd.DataFrame(results).sort_values(\"RMSE\")\nprint(eval_df)\n\n# -----------------------------\n# Plot: actual vs forecasts\n# -----------------------------\nplt.figure()\nplt.plot(train_y.index, train_y.values, label=\"Train\")\nplt.plot(test_y.index, test_y.values, label=\"Test (Actual)\")\nplt.plot(test_y.index, ets_preds, label=\"ETS Forecast\")\nplt.plot(test_y.index, sarima_preds, label=\"SARIMA Forecast\")\nplt.plot(test_y.index, sarimax_preds, label=\"SARIMAX Forecast\")\nplt.title(\"Phase 4: Walk-Forward Forecasts (12 weeks)\")\nplt.xlabel(\"Week\")\nplt.ylabel(\"Total Weekly Sales\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -----------------------------\n# Save forecasts to Snowflake table\n# -----------------------------\nforecast_out = pd.DataFrame({\n    \"DS\": test_y.index,\n    \"Y_ACTUAL\": test_y.values,\n    \"Y_ETS\": ets_preds,\n    \"Y_SARIMA\": sarima_preds,\n    \"Y_SARIMAX\": sarimax_preds\n})\n\nsession.create_dataframe(forecast_out).write.mode(\"overwrite\").save_as_table(\"STORE_WEEKLY_FORECASTS_PHASE4\")\n\n\"Saved to Snowflake table: STORE_WEEKLY_FORECASTS_PHASE4\"\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac31ada0-e7f0-4850-8a64-aa4c6b1ebb46",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "display(eval_df)\n\nbest_row = eval_df.iloc[0]\nbest_model = best_row[\"model\"]\nbest_rmse = best_row[\"RMSE\"]\nbest_mae = best_row[\"MAE\"]\nbest_mape = best_row[\"MAPE\"]\n\nprint(\"Best model (lowest RMSE):\", best_model)\nprint(f\"RMSE: {best_rmse:,.2f} | MAE: {best_mae:,.2f} | MAPE: {best_mape:.2f}%\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1fc65f3-1bd8-4571-97d4-6ecc91244b7c",
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "source": "Phase 5"
  },
  {
   "cell_type": "code",
   "id": "3765ec21-c7d6-473e-b28e-cb1a8dd8bc32",
   "metadata": {
    "language": "python",
    "name": "cell41",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timezone\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# ----------------------------\n# Setup\n# ----------------------------\ndf = df.copy()\ndf.index = pd.to_datetime(df.index)\ndf = df.sort_index()\n\nexog_cols = [\"IS_HOLIDAY\", \"TEMPERATURE\", \"FUEL_PRICE\", \"CPI\", \"UNEMPLOYMENT\"]\n\ny = df[\"Y\"].asfreq(\"W-FRI\").interpolate(\"time\").ffill().bfill()\nX = df[exog_cols].asfreq(\"W-FRI\").interpolate(\"time\").ffill().bfill()\n\ny.index.freq = \"W-FRI\"\nX.index.freq = \"W-FRI\"\n\nh = 12  # forecast horizon\n\n# ----------------------------\n# Pipeline function\n# ----------------------------\ndef run_forecast_pipeline(y, X, horizon=12, scenario_name=\"BASELINE\", scenario_exog_future=None):\n    order = (1, 0, 1)\n    seasonal_order = (0, 1, 0, 52)\n\n    model = SARIMAX(\n        y,\n        exog=X,\n        order=order,\n        seasonal_order=seasonal_order,\n        enforce_stationarity=False,\n        enforce_invertibility=False\n    )\n    fit = model.fit(disp=False)\n\n    last_date = y.index.max()\n    future_index = pd.date_range(start=last_date + pd.Timedelta(weeks=1), periods=horizon, freq=\"W-FRI\")\n    future_index.freq = \"W-FRI\"\n\n    if scenario_exog_future is None:\n        last_row = X.iloc[-1]\n        exog_future = pd.DataFrame([last_row.values] * horizon, columns=X.columns, index=future_index)\n    else:\n        exog_future = scenario_exog_future.copy()\n        exog_future.index = future_index\n        exog_future = exog_future[X.columns]  # enforce same column order\n\n    yhat = fit.forecast(steps=horizon, exog=exog_future)\n    yhat = pd.Series(yhat.values, index=future_index, name=\"Y_PRED\")\n\n    run_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n    out = pd.DataFrame({\n        \"RUN_TS_UTC\": run_ts,\n        \"SCENARIO\": scenario_name,\n        \"DS\": yhat.index,\n        \"Y_PRED\": yhat.values\n    })\n\n    for col in exog_future.columns:\n        out[col] = exog_future[col].values\n\n    coef_tbl = pd.DataFrame({\n        \"PARAMETER\": fit.params.index,\n        \"COEF\": fit.params.values\n    })\n\n    return out, coef_tbl\n\n\n# ----------------------------\n# 1) Baseline run\n# ----------------------------\nbaseline_out, coef_tbl = run_forecast_pipeline(y, X, horizon=h, scenario_name=\"BASELINE\")\n\n# Clean interpretability table (only exogenous coefficients)\ncoef_tbl_exog = coef_tbl[coef_tbl[\"PARAMETER\"].isin(exog_cols)].copy()\ncoef_tbl_exog[\"ABS_COEF\"] = coef_tbl_exog[\"COEF\"].abs()\ncoef_tbl_exog = coef_tbl_exog.sort_values(\"ABS_COEF\", ascending=False).drop(columns=[\"ABS_COEF\"])\n\n# ----------------------------\n# 2) What-if scenario: Economic downturn (raise unemployment)\n# ----------------------------\nfuture_index = pd.date_range(start=y.index.max() + pd.Timedelta(weeks=1), periods=h, freq=\"W-FRI\")\nfuture_index.freq = \"W-FRI\"\n\nlast_row = X.iloc[-1]\necon_exog = pd.DataFrame([last_row.values] * h, columns=X.columns, index=future_index)\n\n# Scenario: unemployment increases by +1.0 percentage point\necon_exog[\"UNEMPLOYMENT\"] = econ_exog[\"UNEMPLOYMENT\"] + np.linspace(0.2, 1.0, h)\n\necon_out, _ = run_forecast_pipeline(y, X, horizon=h, scenario_name=\"UNEMPLOYMENT_PLUS_1PT\", scenario_exog_future=econ_exog)\n\n# Combine\nphase5_out = pd.concat([baseline_out, econ_out], ignore_index=True)\n\n# Delta table\ndelta = econ_out[[\"DS\", \"Y_PRED\"]].merge(\n    baseline_out[[\"DS\", \"Y_PRED\"]],\n    on=\"DS\",\n    suffixes=(\"_ECON\", \"_BASE\")\n)\ndelta[\"DELTA_ECON_MINUS_BASE\"] = delta[\"Y_PRED_ECON\"] - delta[\"Y_PRED_BASE\"]\n\nprint(\"Phase 5: automated pipeline + scenario simulation.\")\nprint(\"\\nExogenous coefficients (interpretability):\")\ndisplay(coef_tbl_exog)\n\nprint(\"\\nScenario impact preview (Unemployment +1pt minus Baseline):\")\ndisplay(delta.head())\n\n# ----------------------------\n# Save outputs to Snowflake\n# ----------------------------\nsession.create_dataframe(phase5_out).write.mode(\"overwrite\").save_as_table(\"STORE_WEEKLY_FORECASTS_PHASE5\")\nsession.create_dataframe(delta).write.mode(\"overwrite\").save_as_table(\"STORE_WEEKLY_SCENARIO_DELTA_PHASE5\")\n\nprint(\"\\nSaved tables:\")\nprint(\"- STORE_WEEKLY_FORECASTS_PHASE5\")\nprint(\"- STORE_WEEKLY_SCENARIO_DELTA_PHASE5\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a9f6431-f70d-4389-bdf1-af551c24221e",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "Phase 6 \n\nThe 12-week forecast for Store 1 should be treated as an operating plan for weekly labor, inventory replenishment, and promotional pacing. Operationally, the forecast provides a baseline expectation for weekly sales so the store can schedule staffing to match expected demand and avoid overstaffing during softer weeks or understaffing during stronger weeks. Inventory teams can use the forecast as a replenishment signal: when predicted weekly sales are higher, safety stock and order quantities should be increased slightly ahead of time to prevent stockouts, while lower predicted weeks can be used to reduce inbound volume and minimize carrying costs. From a retail execution standpoint, the forecast also supports calendar planning by identifying when sales are likely to be stronger and aligning promotion timing, end-cap space, and store readiness (labor coverage, truck unloading windows, and shelf availability) to those weeks.\n\nThe “what-if” scenario simulation adds a practical layer for planning under uncertainty. In the unemployment scenario (gradually increasing up to +1.0 point), the model indicates a proportional reduction in predicted sales relative to the baseline. This can be used as a contingency plan: if macroeconomic conditions deteriorate, leadership can proactively tighten purchasing, shift promotional strategy toward value-focused offers, and adjust labor budgets modestly to protect margin while maintaining customer experience. The key takeaway is not that unemployment causes an exact dollar change every week in real life, but that the model provides a structured way to stress-test the plan and quantify “how bad could it get” under a realistic external shift.\n\nDecision support narrative\n\nFor stakeholders, the forecast and scenario outputs function as a weekly decision dashboard. Store leadership can use the baseline forecast as the primary weekly target for sales and staffing. The finance/planning view is to translate the forecast into budget guardrails (expected revenue band) and monitor weekly variance, especially in the final 12-week window where decisions are most actionable. For merchandising and operations, the forecast should drive a simple cadence: review next 4 weeks every week, confirm replenishment timing, and adjust labor schedules one to two weeks ahead based on the forecast direction and any known events. The scenario analysis supports executive conversations by providing a “Plan A vs Plan B” narrative: Plan A assumes conditions stay stable; Plan B assumes a measurable macro shift and recommends specific cost and inventory responses to reduce risk.\n\nRisks and limitations\n\nThis forecast is only as strong as the data and assumptions behind it. First, the horizon is short (12 weeks) and the model relies on recent patterns; unexpected events, supply constraints, or sudden competitive changes can break historical relationships. Second, the scenario analysis assumes other drivers stay constant while unemployment changes; in reality, multiple variables move together and store-level customer behavior may respond nonlinearly. Third, some features may show weak effects (for example, the holiday flag was effectively near zero in the fitted model), which can happen when the signal is limited or already captured through seasonality; this means not every business intuition will appear as a strong coefficient. Finally, because the modeling was built for Store 1, the results should not automatically be generalized to other stores without repeating the same workflow and validating performance store-by-store.\n\nGiven these limitations, the recommendation is to treat the forecast as a decision aid rather than a guarantee, and to operationalize it through monitoring. Each week, compare actuals to forecast, track error metrics over time, and revisit the model if errors drift upward or if data conditions change. This keeps the forecasting workflow deployment-ready and aligned with business value"
  }
 ]
}